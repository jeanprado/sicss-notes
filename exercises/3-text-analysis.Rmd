---
title: "day 3: text analysis"
author: "jean prado"
date: "12 jun. 2021"
output:
  html_document:
    theme: "cosmo"
    css: "../style/custom.css"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

os exerc√≠cios aqui s√£o baseados no [dia 3 do SICSS](https://github.com/compsocialscience/summer-institute/tree/master/2020/materials/day3-text-analysis). **nota:** nessa an√°lise estou usando a vers√£o nova do pipe (`|>`, que na [fonte do c√≥digo](https://github.com/tonsky/FiraCode) √© mostrada como <code style="font-family: 'Fira Code'">|></code>) no lugar da vers√£o do `magrittr`, a `%>%`. para mais informa√ß√µes, veja [esse post da Bea](https://beatrizmilz.com/blog/2021-05-18-experimentando-o-r-410/).

## puxa dados do Twitter
o que ser√° que est√£o falando da Amaz√¥nia? puxei 10 mil tweets com a palavra **amaz√¥nia** em portugu√™s de **4 a 12 de junho**.

```{r import}
#do not run - tweets already saved at data-raw/tweets-amazonia.rds
# raw <- rtweet::search_tweets(q = "amaz√¥nia lang:pt", n = 18000,
#                       include_rts = FALSE, type = "mixed")
# raw |> saveRDS(file = "../data-raw/tweets-amazonia.rds")

raw <- readRDS(file = "../data-raw/tweets-amazonia.rds")
```

## pr√©-processamento
para mais informa√ß√µes, veja o [c√≥digo anotado](https://sicss.io/2020/materials/day3-text-analysis/basic-text-analysis/rmarkdown/Basic_Text_Analysis_in_R.html) da aula de pr√©-processamento.

antes at√© de come√ßar o pr√©-processamento, vou remover os links do texto dos tweets. [da primeira vez](https://github.com/jeanprado/sicss-notes/commit/fe9f5ba50b954de0c1b7552b49187637f4a58a11) n√£o fiz isso e em algumas partes da an√°lise apareceu uma sequ√™ncia de texto aleat√≥rio, como `dj1398d928`, que era o final das URLs. mais f√°cil remover esse tipo de coisa antes de tokenizar o texto (quebrar em palavras √∫nicas).

para remover as URLs do texto, usei uma [regex](https://pt.wikipedia.org/wiki/Express%C3%A3o_regular) que adaptei [dessa thread](https://stackoverflow.com/questions/1400431/regular-expression-match-any-word-until-first-space) no stack overflow. (ningu√©m √© de ferro, n√©?)

```{r unnest_tokens}
tweets <- raw |> 
  dplyr::mutate(text = stringr::str_remove_all(text, "https([^\\s]+)"))

tt <- tweets |> dplyr::select(created_at, text) |> 
  tidytext::unnest_tokens("word", text)
```

### remove as stopwords
tirando as URLs antes, n√£o precisei adicionar termos como `https`, `t.co` e similares nas stopwords. üòâ

```{r rm_stopwords}
sw_adicionais <- c("ser", "√©", "pra", "sobre", "q", "vai", "est√°s")

tt <- tt |> dplyr::filter(!word %in% c(tm::stopwords("pt"), sw_adicionais))

tt |> dplyr::count(word, sort=T)
```

### tem acentos?
por enquanto tomei a decis√£o de n√£o filtrar acentos, entendendo que eles s√£o importante pra an√°lise. no entanto, √© importante saber quais palavras t√™m acento -- √© o que o c√≥digo abaixo faz.

```{r}
tt |> dplyr::filter(stringr::str_detect(word, "[√Ä-√∫]")) |> dplyr::count(word, sort=T)
```


### tem pontua√ß√£o/caracacteres especiais? se sim, remover
`\W` √© uma regex para remover caracteres que n√£o s√£o letras (non-word characters)

```{r check_non_word}
tt |> dplyr::filter(stringr::str_detect(word, "\\W")) |> dplyr::count(word, sort=T)
```

opa, tem sim, ent√£o removendo...
```{r rm_non_word_1}
tt |> dplyr::filter(stringr::str_detect(word, "\\W")) |> 
  dplyr::mutate(word = stringr::str_remove_all(word, "\\W")) |> 
  dplyr::count(word, sort=T)
```

epa, pera. percebeu que essa regex removeu tamb√©m o ponto de n√∫meros, a√≠ `4.0` virou `40`? penso que pode ser um problema na an√°lise, porque n√£o representa exatamente a express√£o original. ent√£o vou remover todas as pontua√ß√µes *menos* as que est√£o entre n√∫meros.

pra isso, fiz a regex `^\d+(?:(\.|,)\d+)*$` que adaptei [daqui](https://stackoverflow.com/questions/27141528/regular-expression-that-allow-dots-between-numbers) pra identificar ponto ou v√≠rgula entre n√∫meros. no c√≥digo abaixo, a pontua√ß√£o s√≥ √© removida quando esse padr√£o **n√£o** √© identificado.
```{r rm_non_word_2}
tt |> dplyr::filter(stringr::str_detect(word, "\\W")) |> 
  dplyr::mutate(word = dplyr::if_else(stringr::str_detect(word, "^\\d+(?:(\\.|,)\\d+)*$"), word,
                                      stringr::str_remove_all(word, "\\W"))) |> 
  dplyr::count(word, sort=T)

tt <- tt |>
  dplyr::mutate(word = dplyr::if_else(stringr::str_detect(word, "^\\d+(?:(\\.|,)\\d+)*$"), word,
                                      stringr::str_remove_all(word, "\\W")))
```

### palavras mais representativas

usando o m√©todo de **inverse document frequency (tf-idf)**. a explica√ß√£o do [\@jtrecenti](https://twitter.com/jtrecenti?s=20) nessa [live da curso-r](https://youtu.be/NQwFIZBQrg8) foi a √∫nica que me fez entender at√© agora: a tf-idf **encontra as palavras que mais representam um documento** (s√£o frequentes dentro de um documento e relativamente raras entre os outros documentos).
```{r create_idf}
tt_idf <- tt |> dplyr::count(word, created_at) |>
  tidytext::bind_tf_idf(word, created_at, n)

tt_idf |> dplyr::arrange(desc(tf_idf))
```

hmm, suspeito que essas palavras mais raras s√£o na verdade as @s de usu√°rios com poucas intera√ß√µes/tweets sobre amaz√¥nia. ent√£o, antes de rodar o algoritmo da tf-idf, vou remover @men√ß√µes a outros usu√°rios do texto dos tweets usando a regex `@\\W+`:
  
  (depois que rodei, vi que pegava algumas hashtags irrelevantes, ent√£o vou remov√™-las tamb√©m. n√£o seria o padr√£o para todos os casos -- pode ser desej√°vel identificar hashtags pouco usadas ou que podem ser relevantes em um t√≥pico, mas n√£o √© o que quero aqui.)

((outra coisa que fiz aqui foi incluir o `status_id` como identificador no lugar do `created_at`, porque estava dando peso maior a palavras repetidas dentro de um mesmo tweet. na verdade eu s√≥ coloquei `created_at` no come√ßo para seguir o autor do exerc√≠cio.))
```{r remake_idf}
# recria o dataset tidytext (tt) sem considerar @men√ß√µes a outros usu√°rios
tt <- tweets |> dplyr::select(user_id, text) |> 
  dplyr::mutate(text = stringr::str_remove_all(text, "(@|#)\\w+"),
                dplyr::across(text, stringr::str_squish)) |>
  tidytext::unnest_tokens("word", text) |> # repetindo o filtro das stopwords
  dplyr::filter(!word %in% c(tm::stopwords("pt"), sw_adicionais))

# identifica e @ dos usu√°rios que s√£o mencionados no dataset
# (se existe algum jeito menos burro de combinar m√∫ltiplas colunas em um vetor de caracteres, desconhe√ßo)
users <- tweets |> dplyr::select(screen_name, reply_to_screen_name, quoted_screen_name) |>
  tidyr::unite("users", sep = "~") |> tidyr::separate_rows(users, sep = "~") |> 
  dplyr::filter(!users %in% c("NA", "")) |> dplyr::distinct() |> dplyr::pull(users)

tt_idf <- tt |> dplyr::count(word, user_id) |>
  dplyr::filter(!word %in% users) |> 
  tidytext::bind_tf_idf(word, user_id, n)

tt_idf |> dplyr::arrange(desc(tf_idf))
```

aiai.

### cria a document-term matrix
agora que j√° temos as palavras tratadas, o √∫ltimo passo antes de estruturar para a an√°lise seria fazer o _stemming_ (reduzir as palavras √† forma mais b√°sica). mas tentei aqui (no c√≥digo abaixo) e n√£o funcionou muito bem no portugu√™s üòï

```{r stemming}
tt |> dplyr::mutate(word = SnowballC::wordStem(words = word, language = "pt")) |> 
  dplyr::count(word, sort=T)
```

ent√£o vou ser rebelde e fazer a document-term matrix sem o stemming. essa estrutura √© √∫til para a modelagem de t√≥picos

```{r create_dtm}
tt_dtm <- tt |> dplyr::count(user_id, word) |>
  tidytext::cast_dtm(user_id, word, n)

tt_dtm
```

### modelagem de t√≥picos com LDA

agora que a divers√£o come√ßa. usando o pacote `topicmodels` e seguindo as instru√ß√µes [dessa aula aqui](https://sicss.io/2020/materials/day3-text-analysis/topic-modeling/rmarkdown/Topic_Modeling.html), usando Latent Dirichlet Allocation (LDA)

```{r lda, fig.width=12, fig.height=7}
tt_model <- topicmodels::LDA(tt_dtm, k = 6,
                             control = list(seed = 123))

model_topics <- tidytext::tidy(tt_model, matrix = "beta")

model_top_terms <- model_topics |> dplyr::group_by(topic) |> 
  dplyr::top_n(10, beta) |> dplyr::ungroup() |> 
  dplyr::arrange(topic, -beta)

model_top_terms |> dplyr::mutate(term = reorder(term, beta)) |> 
  ggplot2::ggplot(ggplot2::aes(term, beta, fill = factor(topic))) +
  ggplot2::geom_col(show.legend = FALSE) +
  ggplot2::facet_wrap(~ topic, scales = "free") +
  ggplot2::coord_flip()
```

### structural topic modeling

LDA n√£o √© a √∫nica forma de topic modeling, apesar de ser a mais comum. uma alternativa √© a structural topic modeling (STM), que, al√©m de olhar para as palavras, considera metadados aos quais elas est√£o associadas -- consequentemente, a rela√ß√£o entre esssas vari√°veis e os t√≥picos em si.

ainda quero testar com tweets (√© poss√≠vel? quais seriam outras covari√°veis? autores? a intera√ß√£o entre os autores? como operacionalizar isso?), ent√£o por enquanto deixo algumas refer√™ncias para se aprofundar:
  
  - [SICSS - Structural Topic Modeling](https://sicss.io/2020/materials/day3-text-analysis/topic-modeling/rmarkdown/Topic_Modeling.html)
- [Using Structural Topic Modeling to Detect Events and Cluster Twitter Users in the Ukrainian Crisis](https://link.springer.com/chapter/10.1007/978-3-319-21380-4_108)
