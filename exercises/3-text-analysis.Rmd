---
title: "day 3: text analysis"
author: "jean prado"
date: "12 jun. 2021"
output:
  html_document:
    theme: "cosmo"
    css: "../style/custom.css"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

os exerc√≠cios aqui s√£o baseados no [dia 3 do SICSS](https://github.com/compsocialscience/summer-institute/tree/master/2020/materials/day3-text-analysis). **nota:** nessa an√°lise estou usando a vers√£o nova do pipe (`|>`, que na [fonte do c√≥digo](https://github.com/tonsky/FiraCode) √© mostrada como <code style="font-family: 'Fira Code'">|></code>) no lugar da vers√£o do `magrittr`, a `%>%`. para mais informa√ß√µes, veja [esse post da Bea](https://beatrizmilz.com/blog/2021-05-18-experimentando-o-r-410/).

## puxa dados do Twitter
o que ser√° que est√£o falando da Amaz√¥nia? puxei 10 mil tweets com a palavra **amaz√¥nia** em portugu√™s de **4 a 12 de junho**.

```{r import}
#do not run - tweets already saved at data-raw/tweets-amazonia.rds
# raw <- rtweet::search_tweets(q = "amaz√¥nia lang:pt", n = 18000,
#                       include_rts = FALSE, type = "mixed")
# raw |> saveRDS(file = "../data-raw/tweets-amazonia.rds")

raw <- readRDS(file = "../data-raw/tweets-amazonia.rds")
```

## pr√©-processamento
para mais informa√ß√µes, veja o [c√≥digo anotado](https://sicss.io/2020/materials/day3-text-analysis/basic-text-analysis/rmarkdown/Basic_Text_Analysis_in_R.html) da aula de pr√©-processamento.

antes at√© de come√ßar o pr√©-processamento, vou remover os links do texto dos tweets. [da primeira vez](https://github.com/jeanprado/sicss-notes/commit/fe9f5ba50b954de0c1b7552b49187637f4a58a11) n√£o fiz isso e em algumas partes da an√°lise apareceu uma sequ√™ncia de aleat√≥rio, como `dj1398d928`, que era o final das URLs. mais f√°cil remover esse tipo de coisa antes de tokenizar o texto (quebrar em palavras √∫nicas).

para remover as URLs do texto, usei uma [regex](https://pt.wikipedia.org/wiki/Express%C3%A3o_regular) que copiei dessa thread no stack overflow. (ningu√©m √© de ferro, n√©?)

```{r unnest_tokens}
tweets <- raw |> 
  dplyr::mutate(text = stringr::str_remove_all(text, "(https?:\\/\\/)?([\\w\\-])+\\.{1}([a-zA-Z]{2,63})*?([^#\\n\\r]*)?"))

tt <- tweets |> dplyr::select(created_at, text) |> 
  tidytext::unnest_tokens("word", text)
```

### remove as stopwords
tirando as URLs antes, n√£o precisei adicionar termos como `https`, `t.co` e similares nas stopwords. üòâ

```{r rm_stopwords}
tt <- tt |> dplyr::filter(!word %in% c(tm::stopwords("pt"),
                                       "ser", "√©", "pra", "sobre", "q", "vai"))

tt |> dplyr::count(word, sort=T)
```

### tem acentos?
por enquanto tomei a decis√£o de n√£o filtrar acentos, entendendo que eles s√£o importante pra an√°lise. no entanto, √© importante saber quais palavras t√™m acento -- √© o que o c√≥digo abaixo faz.

```{r}
tt |> dplyr::filter(stringr::str_detect(word, "[√Ä-√∫]")) |> dplyr::count(word, sort=T)
```


### tem pontua√ß√£o/caracacteres especiais? se sim, remover
`\W` √© uma regex para remover caracteres que n√£o s√£o letras (non-word characters)

```{r check_non_word}
tt |> dplyr::filter(stringr::str_detect(word, "\\W")) |> dplyr::count(word, sort=T)
```

opa, tem sim, ent√£o removendo...
```{r rm_non_word_1}
tt |> dplyr::filter(stringr::str_detect(word, "\\W")) |> 
  dplyr::mutate(word = stringr::str_remove_all(word, "\\W")) |> 
  dplyr::count(word, sort=T)
```

epa, pera. percebeu que essa regex removeu tamb√©m o ponto de n√∫meros, a√≠ `4.0` virou `40`? penso que pode ser um problema na an√°lise, porque n√£o representa exatamente a express√£o original. ent√£o vou remover todas as pontua√ß√µes *menos* as que est√£o entre n√∫meros.

pra isso, fiz a regex `^\d+(?:(\.|,)\d+)*$` que adaptei [daqui](https://stackoverflow.com/questions/27141528/regular-expression-that-allow-dots-between-numbers) pra identificar ponto ou v√≠rgula entre n√∫meros. no c√≥digo abaixo, a pontua√ß√£o s√≥ √© removida quando esse padr√£o **n√£o** √© identificado.
```{r rm_non_word_2}
tt |> dplyr::filter(stringr::str_detect(word, "\\W")) |> 
  dplyr::mutate(word = dplyr::if_else(stringr::str_detect(word, "^\\d+(?:(\\.|,)\\d+)*$"), word,
                                      stringr::str_remove_all(word, "\\W"))) |> 
  dplyr::count(word, sort=T)

tt <- tt |>
  dplyr::mutate(word = dplyr::if_else(stringr::str_detect(word, "^\\d+(?:(\\.|,)\\d+)*$"), word,
                                      stringr::str_remove_all(word, "\\W")))
```

### palavras incomuns?

inverse document frequency: √∫til para palavras pouco frequentes, mas aqui n√£o mostrou muita coisa
```{r create_idf}
tt_idf <- tt |> dplyr::count(word, created_at) |>
  tidytext::bind_tf_idf(word, created_at, n)

tt_idf |> dplyr::arrange(desc(tf_idf))
```

### cria a document-term matrix
agora que j√° temos as palavras tratadas, o √∫ltimo passo antes de estruturar para a an√°lise seria fazer o _stemming_ (reduzir as palavras √† forma mais b√°sica). mas tentei aqui (no c√≥digo abaixo) e n√£o funcionou muito bem no portugu√™s üòï

```{r stemming}
tt |> dplyr::mutate(word = SnowballC::wordStem(words = word, language = "pt")) |> 
  dplyr::count(word, sort=T)
```

ent√£o vou ser rebelde e fazer a document-term matrix sem o stemming. essa estrutura √© √∫til para a modelagem de t√≥picos

```{r create_dtm}
tt_dtm <- tt |> dplyr::count(created_at, word) |>
  tidytext::cast_dtm(created_at, word, n)

tt_dtm
```

### modelagem de t√≥picos com LDA

agora que a divers√£o come√ßa. usando o pacote `topicmodels` e seguindo as instru√ß√µes [dessa aula aqui](https://sicss.io/2020/materials/day3-text-analysis/topic-modeling/rmarkdown/Topic_Modeling.html), usando Latent Dirichlet Allocation (LDA)

```{r lda, fig.width=12, fig.height=7}
tt_model <- topicmodels::LDA(tt_dtm, k = 9,
                             control = list(seed = 123))

model_topics <- tidytext::tidy(tt_model, matrix = "beta")

model_top_terms <- model_topics |> dplyr::group_by(topic) |> 
  dplyr::top_n(10, beta) |> dplyr::ungroup() |> 
  dplyr::arrange(topic, -beta)

model_top_terms |> dplyr::mutate(term = reorder(term, beta)) |> 
  ggplot2::ggplot(ggplot2::aes(term, beta, fill = factor(topic))) +
  ggplot2::geom_col(show.legend = FALSE) +
  ggplot2::facet_wrap(~ topic, scales = "free") +
  ggplot2::coord_flip()
```

### structural topic modeling

LDA n√£o √© a √∫nica forma de topic modeling, apesar de ser a mais comum. uma alternativa √© a structural topic modeling (STM), que, al√©m de olhar para as palavras, considera metadados aos quais elas est√£o associadas -- consequentemente, a rela√ß√£o entre esssas vari√°veis e os t√≥picos em si.

ainda quero testar com tweets (√© poss√≠vel? quais seriam outras covari√°veis? autores? a intera√ß√£o entre os autores? como operacionalizar isso?), ent√£o por enquanto deixo algumas refer√™ncias para se aprofundar:

- [SICSS - Structural Topic Modeling](https://sicss.io/2020/materials/day3-text-analysis/topic-modeling/rmarkdown/Topic_Modeling.html)
- [Using Structural Topic Modeling to Detect Events and Cluster Twitter Users in the Ukrainian Crisis](https://link.springer.com/chapter/10.1007/978-3-319-21380-4_108)
